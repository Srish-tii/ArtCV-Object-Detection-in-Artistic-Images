## Introduction

Conventional deep neural network models implemented for object detection in images tend to demonstrate subpar performance when it comes to art, especially in paintings and drawings. Because most object detection models are trained on large datasets of photographs and have been optimized to recognize an object by its texture rather than shape, the problem of object recognition in art arises from the unique styles and distortions made by brush strokes and manually drawn lines. To improve object detection, we have fine-tuned neural network models on stylized images generated by applying state-of-the-art style transfer techniques to existing datasets. Such modifications would help better align the two domains, improving object detection performance in art.

## Motivation

In tandem with the sweeping advances made in the field of object detection over the past decade, the demand to expand its use beyond photographs has also grown. In the realm of art, for instance, art museums have started using computer vision to help categorize or annotate the vast number of pieces in their collections in order to ease the burden of doing so by hand. 
We could fine-tune a neural network, but there are not enough datasets of labeled artwork. Kadish et al. (2021) suggest using style transfer on labeled photographs to generate labeled art. Hence, to contribute to this development, we aim to re-implement the research (Kadish et al., 2021), in which the authors employed an art style transfer technique on an already existing and labeled dataset of photographs to then fine-tune a neural network to better identify people in real artwork. 
Since the development of the dataset in (Kadish et al., 2021), it was found that style transfer models suffer from what (An et al., 2021) term as “content leak”. Through repeated rounds of styling an input image based on a reference image, the content from the reference image begins to appear in the stylized input image. Modern style transfer models attempt to solve the issue of content leak. 
Finally, we also generate alternative training sets with improved style transfer models, Whitening and Color Transforms (WCT) and SANet, and compare results of people detection with baseline AdaIN.

## to edit from here - 

## Steps to build StyleCOCO

StyleCOCO is generated from two image sets. `COCO 2017` contains the content images and `Painter By Numbers` contains the style images.

**COCO 2017**: On the linux server, `wget http://images.cocodataset.org/zips/train2017.zip` and unzip into `content_dir`

**Painter By Numbers**: This requires the Kaggle API. 

- Create a Kaggle account. 
- Go to `https://www.kaggle.com/settings`
- Go to API section, click `Create New Token`. Follow instructions provided to place JSON file in appropriate location on linux server
- Join Painter By Numbers competition: `https://www.kaggle.com/c/painter-by-numbers/`
- Run `kaggle competitions download -c painter-by-numbers -f train.zip -p /path/to/style_dir`


**Create StyleCOCO**: From the root project folder, `cd models/stylize` and run `nohup python3 stylize.py --content-dir '../../data/content_dir/' --style-dir '../../data/style_dir/' --output-dir '../../output/styled_images' --num-styles 1 --alpha 1 --content-size 0 --style-size 0 --crop 0 &`

